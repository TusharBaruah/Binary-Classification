{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 13 - IDC 409 Term Project\n",
    "\n",
    "Members:\n",
    "* Tushar Baruah (MS19015)\n",
    "* Atharva Hingane (MS19043)\n",
    "* Nitish (MS19101)\n",
    "\n",
    "### Project 1: Continuum Supression in HEP\n",
    "\n",
    "We need to classify 'type' 0 and 1 in our data as signal and the others as background.\n",
    "\n",
    "1. Our data has 59 independent parameters. We perform PCA to first reduce them.\n",
    "2. We perform logistic regression to classify the data as signal or background.\n",
    "\n",
    "data URL = <https://drive.google.com/file/d/1wceoRWDkqdXrAAt6x6Kuf9Dv3alW3iad/view?usp=drive_link>\n",
    "\n",
    "GITHUB link = <https://github.com/TusharBaruah/Binary-Classification>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before PCA: (70606, 59) \n",
      "After PCA: (70606, 32)\n",
      "X_train: (49424, 32)\n",
      "X_test: (21182, 32)\n",
      "Y_train: (49424,)\n",
      "Y_test: (21182,)\n"
     ]
    }
   ],
   "source": [
    "# Importing data and performing PCA\n",
    "\n",
    "# data URL = https://drive.google.com/file/d/1wceoRWDkqdXrAAt6x6Kuf9Dv3alW3iad/view?usp=drive_link\n",
    "# GITHUB link = https://github.com/TusharBaruah/Binary-Classification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Data = pd.read_csv('data_hep.csv')\n",
    "Y_labels = Data['type'].to_numpy()\n",
    "\n",
    "# re-labelling the data to signal (1) and background (0)\n",
    "for i in range(0,len(Y_labels)):\n",
    "    if(Y_labels[i]==0 or Y_labels[i]==1):\n",
    "        Y_labels[i]=1.0 # signal\n",
    "    else:\n",
    "        Y_labels[i]=0.0 # background\n",
    "\n",
    "Data = Data.drop('type',axis=1) # removing previous 'type' label\n",
    "Data['type']=Y_labels # adding the signal and background label\n",
    "X_set = Data.drop(columns=[\"Unnamed: 0\",\"type\"])\n",
    "X_set = X_set.to_numpy()\n",
    "\n",
    "pca= PCA(n_components=32)\n",
    "result=pca.fit(X_set)\n",
    "X_pca= result.transform(X_set)\n",
    "print(\"Before PCA:\",X_set.shape,\"\\nAfter PCA:\",X_pca.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y_labels, test_size = 0.3, random_state=0)\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"Y_train:\",Y_train.shape)\n",
    "print(\"Y_test:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tushar Baruah\\AppData\\Local\\Temp\\ipykernel_2668\\4291429584.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_vec = np.array([- (y[i] * np.log(y_dash[i])) - ((1 - y[i]) * np.log(1 - y_dash[i])) for i in range(m)])\n",
      "C:\\Users\\Tushar Baruah\\AppData\\Local\\Temp\\ipykernel_2668\\4291429584.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss_vec = np.array([- (y[i] * np.log(y_dash[i])) - ((1 - y[i]) * np.log(1 - y_dash[i])) for i in range(m)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1999 ,Cost: nan\n",
      "Time taken by the algorithm= 260.42791986465454 seconds\n",
      "Training:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83     24247\n",
      "           1       0.83      0.85      0.84     25177\n",
      "\n",
      "    accuracy                           0.83     49424\n",
      "   macro avg       0.83      0.83      0.83     49424\n",
      "weighted avg       0.83      0.83      0.83     49424\n",
      "\n",
      "Testing:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83     10594\n",
      "           1       0.82      0.84      0.83     10588\n",
      "\n",
      "    accuracy                           0.83     21182\n",
      "   macro avg       0.83      0.83      0.83     21182\n",
      "weighted avg       0.83      0.83      0.83     21182\n",
      "\n",
      "Confusion matrix\n",
      " [[8682 1912]\n",
      " [1672 8916]]\n",
      "True Positives(TP) =  8682\n",
      "True Negatives(TN) =  8916\n",
      "False Positives(FP) =  1912\n",
      "False Negatives(FN) =  1672\n"
     ]
    }
   ],
   "source": [
    "# Performing Logistic Regression\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.float64(x)\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "# Function to compute cost  - using vectorization\n",
    "def cost_logreg_vec(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    if(len(y)!=m or len(w) !=n):\n",
    "        print(\"Dataset array and Labels/weights array size does not match\")\n",
    "    z = np.matmul(X, w) + (b * np.ones(m))\n",
    "    y_dash = sigmoid(z)\n",
    "    loss_vec = np.array([- (y[i] * np.log(y_dash[i])) - ((1 - y[i]) * np.log(1 - y_dash[i])) for i in range(m)])\n",
    "    cost = np.dot(loss_vec, np.ones(m)) / m\n",
    "    # cost = cost_func_vec(y, y_dash)\n",
    "    return cost\n",
    "\n",
    "def grad_logreg_vec(X, y, w, b): \n",
    "    m, n = X.shape\n",
    "    if(len(y)!=m or len(w) !=n):\n",
    "        print(\"Dataset array and Labels/weights array size does not match\")\n",
    "    y_dash = sigmoid(np.matmul(X, w) + b * np.ones(m))\n",
    "    grad_w = np.matmul(y_dash - y, X) / m\n",
    "    grad_b = np.dot(y_dash - y, np.ones(m)) / m\n",
    "    \n",
    "    return grad_w, grad_b\n",
    "\n",
    "# Gradient descent algorithm for logistic regression\n",
    "def grad_desc(X, y, w, b, learning_rate, n_iter, show_cost = True): \n",
    "    m, n = X.shape\n",
    "    if(len(y)!=m or len(w) !=n):\n",
    "        print(\"Dataset array and Labels/weights array size does not match\")\n",
    "    cost_history, params_history = [], []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        grad_w, grad_b = grad_logreg_vec(X, y, w, b)   \n",
    "        w += - learning_rate * grad_w\n",
    "        b += - learning_rate * grad_b\n",
    "        cost =  cost_logreg_vec(X, y, w, b)\n",
    "        cost_history.append(cost)\n",
    "        params_history.append([w, b])\n",
    "        if show_cost == True and (i == n_iter - 1):\n",
    "            print(\"Iteration\",i,\",Cost:\",float(cost_history[i]))\n",
    "        \n",
    "    return w, b, cost_history, params_history\n",
    "\n",
    "\n",
    "# Learning model parameters using gradient descent algorithm\n",
    "a=np.zeros(X_train.shape[1])\n",
    "\n",
    "w_out, b_out, cost_history, params_history = grad_desc(X_train,\n",
    "                                                       Y_train,\n",
    "                                                       a,\n",
    "                                                       0,\n",
    "                                                       learning_rate = 0.1,\n",
    "                                                       n_iter = 2000)\n",
    "\n",
    "# Prediction and evaluation on the training set and the test set\n",
    "y_train_prob = sigmoid(np.matmul(X_train, w_out) + (b_out * np.ones(X_train.shape[0])))\n",
    "y_test_prob = sigmoid(np.matmul(X_test, w_out) + (b_out * np.ones(X_test.shape[0])))\n",
    "y_train_pred, y_test_pred = (y_train_prob > 0.5).astype(int), (y_test_prob > 0.5).astype(int)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken by the algorithm=\",end-start,\"seconds\")\n",
    "\n",
    "print(\"Training:\\n\",classification_report(Y_train, y_train_pred))\n",
    "print(\"Testing:\\n\",classification_report(Y_test, y_test_pred))\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print('Confusion matrix\\n', cm)\n",
    "print('True Positives(TP) = ', cm[0,0])\n",
    "print('True Negatives(TN) = ', cm[1,1])\n",
    "print('False Positives(FP) = ', cm[0,1])\n",
    "print('False Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with other existing models\n",
    "\n",
    "K-nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by the algorithm= 0.005001544952392578 seconds\n",
      "score on train: 0.8701845257364843\n",
      "score on test: 0.8148427910490039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y_labels, test_size = 0.3, random_state=0)\n",
    "knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "knn.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "print(\"Time taken by the algorithm=\",end-start,\"seconds\")\n",
    "\n",
    "print(\"Shape of test\",X_test.shape)\n",
    "print(\"score on train: \"+ str(knn.score(X_train, Y_train)))\n",
    "print(\"score on test: \" + str(knn.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-built logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by the algorithm= 1.9510478973388672 seconds\n",
      "Shape of test (21182, 32)\n",
      "score on train: 0.8379329880220135\n",
      "score on test: 0.8368426022094231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y_labels, test_size = 0.3, random_state=0)\n",
    "\n",
    "start = time.time()\n",
    "lr=LogisticRegression(max_iter=5000)\n",
    "lr.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "print(\"Time taken by the algorithm=\",end-start,\"seconds\")\n",
    "\n",
    "print(\"Shape of test\",X_test.shape)\n",
    "print(\"score on train: \"+ str(lr.score(X_train, Y_train)))\n",
    "print(\"score on test: \" + str(lr.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by the algorithm= 3.494051694869995 seconds\n",
      "Shape of test (21182, 32)\n",
      "score on train: 0.8234663321463257\n",
      "score on test: 0.8256066471532433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y_labels, test_size = 0.3, random_state=0)\n",
    "\n",
    "start = time.time()\n",
    "svm=LinearSVC(C=0.0001)\n",
    "svm.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "print(\"Time taken by the algorithm=\",end-start,\"seconds\")\n",
    "\n",
    "print(\"Shape of test\",X_test.shape)\n",
    "print(\"score on train: \"+ str(svm.score(X_train, Y_train)))\n",
    "print(\"score on test: \" + str(svm.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by the algorithm= 0.8829154968261719 seconds\n",
      "Shape of test (21182, 32)\n",
      "score on train: 0.7602379410812561\n",
      "score on test: 0.7552639033141346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y_labels, test_size = 0.3, random_state=0)\n",
    "\n",
    "start = time.time()\n",
    "clf = DecisionTreeClassifier(min_samples_split=10,max_depth=3)\n",
    "clf.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "print(\"Time taken by the algorithm=\",end-start,\"seconds\")\n",
    "\n",
    "print(\"Shape of test\",X_test.shape)\n",
    "print(\"score on train: \" + str(clf.score(X_train, Y_train)))\n",
    "print(\"score on test: \"  + str(clf.score(X_test, Y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
